[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Here I document my learning journeys into various software packages or technical subjects. I also keep this as a sort of open notebook which I find helpful for my own work and for collaboration with colleagues.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\nExtracting Statistics From Figures in R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelational Databases in R, Part 2\n\n\n\n\n\nExample of how “flat” data can be restructured as a three-table relational database using Michigan K-12 education data.\n\n\n\n\n\nNov 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nRunning in the rain\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLearning renv for Reproduciblity\n\n\n… and other benefits\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRelational Databases in R, Part 1\n\n\n\n\n\nData is often hierarchical, structuring it in a relational database is an efficient way to account for this.\n\n\n\n\n\nMay 13, 2023\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/fft_doing_unpleasant_things.html",
    "href": "posts/fft_doing_unpleasant_things.html",
    "title": "Running in the rain",
    "section": "",
    "text": "One afternoon this summer, I found myself running in the rain. It started soon after I did and I was thoroughly soaked before turning around. I like running because it gives me time to reflect, but running in the rain is not a particular pleasant experience. And so I reflected, how did I end up out here with soggy shoes, barely able to see through spotty glasses? The short answer is because I didn’t plan to."
  },
  {
    "objectID": "posts/fft_doing_unpleasant_things.html#self-help-or-self-sabotage",
    "href": "posts/fft_doing_unpleasant_things.html#self-help-or-self-sabotage",
    "title": "Running in the rain",
    "section": "Self-Help or Self-Sabotage?",
    "text": "Self-Help or Self-Sabotage?\nI have enjoyed a fair amount of self-help books and podcasts, such as the OG Seven Habits by Steven Covey. I think that many people have good things to say about time management, self-leadership, etc. One of the most helpful things I have taken away from these resources is the importance of reflection and intentionality. At some point, one needs to choose goals, order their activities towards achieving them, and assess their progress.\nHowever, this can easily be taken too far and result in a never-ending cycle of goal-setting and planning. These meta activities which were once meant to support the achievement of goals actually parasitize the time and energy required to make progress. At worst this can become self-reinforcing when a lack of progress motivates more assessment, reflection, goal-setting, and planning. Thus, planning becomes the end in-and-of itself and self-help becomes self-sabotage."
  },
  {
    "objectID": "posts/fft_doing_unpleasant_things.html#making-progress",
    "href": "posts/fft_doing_unpleasant_things.html#making-progress",
    "title": "Running in the rain",
    "section": "Making Progress",
    "text": "Making Progress\nI fell into this self-sabotage trap during my PhD program. I spent many hours creating spreadsheets to help me plan and assess progress towards goals. I even learned about Gantt charts and tried to make them in R (spoilers: you can, but why?). Only when deadlines started carrying consequences did I prioritize progress and finish the bulk of my dissertation work in about six months.\nMy final dissertation project was the biggest piece and consisted of a large meta-analysis. In a couple months I screened nearly 5000 abstracts and 500 documents, then coded several dozen variables for 150 of those documents. If you have done a meta-analysis then you would know that a meta-analysis with 150 documents (and 450 effect sizes) often takes teams of researchers and students the same or longer amount of time to complete. The only reason why I was able to work long hours efficiently for so long is because the progress I was making was obvious. The progress was itself motivating and spurred me on.\nAt this point, you might say that I benefitted from a great deal of planning that I did do through the course of my program. And yes, it is true that I learned a lot, that I had practiced my skills for years, and that I was able to convert these into a sound project plan. Nevertheless, this planning was not some version of personal management or leadership. I did not budget time, journal my reflections, set internal or intermediate goals, gamify common tasks, or otherwise tinker with my behavior. These things would have stalled my progress and killed my motivation. I did know that I had a sound plan and that I was making progress. Thus, it was relatively easy to get up early and work."
  },
  {
    "objectID": "posts/fft_doing_unpleasant_things.html#when-progress-is-not-apparent",
    "href": "posts/fft_doing_unpleasant_things.html#when-progress-is-not-apparent",
    "title": "Running in the rain",
    "section": "When Progress is Not Apparent",
    "text": "When Progress is Not Apparent\nUnfortunately, it is not always clear when progress is being made. This was certainly true for me in the first few years of my PhD when I did not know what I would do for my dissertation. In these cases, it seems to me that the best thing to do is to develop small, clearly-defined tasks that contribute towards larger goals. For me, some examples of such tasks include browsing the lastest journal issues or writing a blog post like this one.1 Progress can then be defined as completing these tasks on some periodic basis.\nFor example, when I was learning and developing a plan for my disseration, I should have defined progress as reading x papers a week with light notes. Instead of two years or so of haphazard work defined by anxious uncertainty, I would have quickly developed my interests and my capacity to pursue them. As a general rule, I don’t have regrets2 but one of the few I do have is not using this time more effectively."
  },
  {
    "objectID": "posts/fft_doing_unpleasant_things.html#running",
    "href": "posts/fft_doing_unpleasant_things.html#running",
    "title": "Running in the rain",
    "section": "Running",
    "text": "Running\nCompleting a marathon is an aspiration of mine. I do not know when I will, but I would like to in the next few years. I also aspire to live a long, productive life and to be as fit as my Grampa who could outwalk his grandkids in his 80s. I could plan and track progress towards completing a marathon, but I cannot plan and tinker my way into health. Yes, I know, it is important to develop a good lifestyle based on habits conducive to health. But, speaking from personal experience, it is difficult to develop an exercise habit when you are not fit to begin with.\nI ran in the rain that summer day because I didn’t plan to. I had developed the habit of going for a run when I had a free hour in the afternoon. I did not have a plan on which days I would run or at what time. Neither did I complicate my running with gear (just running shoes and earbuds) or routes (just a convenient loop) or target times (just go). These things are not bad and might be really helpful in reaching specific goals. But they can get in the way of making progress."
  },
  {
    "objectID": "posts/fft_doing_unpleasant_things.html#footnotes",
    "href": "posts/fft_doing_unpleasant_things.html#footnotes",
    "title": "Running in the rain",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor me, blogging is a way to exercise my writing skills and to pay back the many blog posts that I have found helpful or interesting.↩︎\nRegrets don’t make sense to me. We are the people we are because of what we have done and experienced. If I had the ability to act on my regrets, changing my past accordingly, then I wouldn’t be the person I am today to regret it. The better question to me is what can I learn, how can I change in light of what I know now?↩︎"
  },
  {
    "objectID": "posts/learning_renv/index.html",
    "href": "posts/learning_renv/index.html",
    "title": "Untitled",
    "section": "",
    "text": "Organizing, managing, and executing reproducible research projects is hard because there are so many possible reasons why a project cannot be reproduced.\n- No data? Not reproducible.\n- No code? Not reproducible.\n- Software is unavailable? Not reproducible.\n- Computational environment is unspecified? Maybe.\n- Software versions are unknown? Eventually no.\nWe think that we have one of these threats to reproducibility whipped, then some unexpected development or complication throws it all back in doubt. I think this is especially true for someone like me: an intermediate R user who has no background in computer science or programming other than what I have learned on my own in the last couple years. This post reflects on my learning how to use renv as part of a reproducible research workflow and how I have handled a couple problems along the way.\n\nWelcome renv\nThe renv package solves (more or less) the software versioning issue. With a couple commands all R package versions are recorded in a file, allowing the R package environment of the project to be rebuilt from scratch. I have used renv for several months now and have appreciated how much it has simplified the task of installing and staying current with package updates. It has also streamlined the storage of R packages on my computer.\nIn a nutshell, here’s how it works. Once renv is installed init() creates a lockfile (renv.lock) that holds package version information and details about how packages were installed. For example, if a package is installed from github, that is recorded along with the username of the repository. When the project is shared, the lockfile can be read by restore() to install all packages exactly as they are recorded. The snapshot() function adds additional packages as they are installed in the project and status() reports packages that need to be installed and/or added to the lockfile. It’s really that easy!\n\n\nFirst Headache\nLearning to use renv and incorporating it into my workflow wasn’t without a couple headaches. At the start, I couldn’t use renv::install()which works similarly to baserenv::install.packages()but is more flexible and intuitive. I would runrenv::install(\"somepackage\") and an error would be returned, to the effect that “package ‘somepackage’ is not available” Yet utils::install.packages(), the “base” package installation function (utils is part of the R distribution), worked just fine.\nApparently many others have had this problem. What it came down to for me is that R and renv were using different download methods. These methods can be checked using getOption(\"download.file.method\") and renv:::renv_download_method() for R and renv respectively. It seems for me (on Windows) that the two available methods are lib and libcurl. These are closely related software libraries/tools created and maintained by the cURL (Client for URLs) project which enable internet file transfers.\nI resolved this problem by including one line in my Rstudio project .Rprofile file: Sys.setenv(RENV_DOWNLOAD_FILE_METHOD = \"libcurl\"). Being in the .Rprofile within the project, this command is always run when loading up the project in Rstudio. A more robust solution that dynamically retrieves the download method currently used by R is what I use now: Sys.setenv(RENV_DOWNLOAD_METHOD = getOption(\"download.file.method\")) (see this stackoverflow question)\n\n\nSecond Headache\nThat first problem seemed to happen again some months later. Again, renv::install() would return an error stating that the package was not available. Apparently, the repository being targeted by renv was the issue. I don’t know what caused it, but I am guessing some update in the backend of renv or its dependencies was responsible.\nThe call getOption(\"repos\") returns the repository currently being used which for me was something like https://packagemanager.posit.co/cran/. I resolved the problem by manually setting the “repos” option in my project’s .Rprofile file. Thus after solving these two headaches, I start new Rstudio projects using renv with my .Rprofile looking like this:\n\nsource(\"renv/activate.R\")\n\nSys.setenv(RENV_DOWNLOAD_FILE_METHOD = getOption(\"download.file.method\"))\n\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\n\n\n\nUnanticipated Benefits\nThe above problems were really not too hard to resolve even for someone with a limited software knowledge base like me. The documentation and support provided by Kevin Ushey, principal developer of renv, and the rest of the Posit team really is superb. I have no doubt that this package will remain stable and functioning going ahead.\nAs I have adopted renv into my normal project workflow I have discovered a few benefits beyond supporting research reproducibility. First, the renv::install() and renv::update() functions are very smooth and intuitive, much better than utils::install.packages(). In particular, I run renv::update() every week or month to automatically update all project packages - including renv itself! Both functions are faster than the utils function and provide more informative errors.\nA second benefit is the efficient caching of R packages on my computer. After the first hundred or so installed packages, the disk space required becomes noticeable. Before renv, I occasionally installed packages twice in different locations and accumulated different versions over time. renv keeps a common cache for the device which it then links to in individual renv projects. This means that for a given package and version, it will only ever be installed once. My computer only has a 200GB hard drive so saving a GB here or there is very nice.\nA third benefit is the quick installation of new-to-me packages. Often dependencies are already included and built in my cache so all renv has to do is link to the cache for that dependency. Thus, downloads and builds from binaries are minimized and new packages are ready to use in mere moments.\nI think all scientists and researchers who use R and who are committed to research reproducibility should adopt renv into their workflow."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "I’m a researcher and educator interested in the critical evaluation of education research and its application to actual students and teachers. I recently graduated with a PhD from the Mallinson Institute for Science Education at Western Michigan University. I am developing a research program which investigates the methods, results, and impact of studies in education to empower teachers and administrators with more robust recommendations.\nI am passionate about education at all levels and the development of knowledge, skills, and character in the next generation. I have taught biology and chemistry at various levels (8th grade to undergraduate) I aspire to be a lifelong learner and enjoy reading of diverse subjects, as well as developing my skills as a researcher.\nSee my CV for a complete listing of my education, work, and experience."
  },
  {
    "objectID": "index.html#key-research-interests",
    "href": "index.html#key-research-interests",
    "title": "About Me",
    "section": "Key research interests",
    "text": "Key research interests\n\nMeta-research methods, such as systematic review and meta-analysis\nComputational reproducibility and open science practices for data collection, analysis, and reporting\nRole of evidence in educational policy and practice"
  },
  {
    "objectID": "index.html#teaching-and-public-engagement-interests",
    "href": "index.html#teaching-and-public-engagement-interests",
    "title": "About Me",
    "section": "Teaching and public engagement interests",
    "text": "Teaching and public engagement interests\n\nScientific methods, experimental design, causal inference, and philosophy of science\nIntroductory and advanced biology subjects, particularly microbiology and molecular biology\nHistory of educational psychology and science education\nStatistical and computational research methods using R, especially computational reproducibility"
  },
  {
    "objectID": "posts/learning_relational_databases_p2/index.html",
    "href": "posts/learning_relational_databases_p2/index.html",
    "title": "Relational Databases in R, Part 2",
    "section": "",
    "text": "In part 1 of this series, the relational model for structuring data was introduced in terms of Hadley Wickham’s tidy data. This post demonstrates how “flat” data can be restructured as a three-table relational database using standard tidyverse tools."
  },
  {
    "objectID": "posts/learning_relational_databases_p2/index.html#introduction-to-dataset",
    "href": "posts/learning_relational_databases_p2/index.html#introduction-to-dataset",
    "title": "Relational Databases in R, Part 2",
    "section": "Introduction to Dataset",
    "text": "Introduction to Dataset\nThe dataset used in this example was downloaded from the Michigan government site MI School Data. It holds contact information and various other data points for all current Michigan public educational entities. All 22 variables are shown with the dplyr::glimpse() function:\n\n\nRows: 5,010\nColumns: 22\n$ SchoolYear      &lt;chr&gt; \"22 - 23 School Year\", \"22 - 23 School Year\", \"22 - 23…\n$ ISDCode         &lt;chr&gt; \"03\", \"03\", \"03\", \"03\", \"03\", \"03\", \"03\", \"03\", \"03\", …\n$ ISDName         &lt;chr&gt; \"Allegan Area Educational Service Agency\", \"Allegan Ar…\n$ DistrictCode    &lt;chr&gt; \"03000\", \"03000\", \"03000\", \"03000\", \"03000\", \"03010\", …\n$ DistrictName    &lt;chr&gt; \"Allegan Area Educational Service Agency\", \"Allegan Ar…\n$ BuildingCode    &lt;chr&gt; \"00000\", \"03816\", \"03816\", \"06730\", \"07254\", \"00000\", …\n$ BuildingName    &lt;chr&gt; \"Allegan Area Educational Service Agency\", \"Early Coll…\n$ COUNTY_CODE     &lt;chr&gt; \"03\", \"03\", \"03\", \"03\", \"03\", \"03\", \"39\", \"03\", \"03\", …\n$ COUNTY_NAME     &lt;chr&gt; \"Allegan\", \"Allegan\", \"Allegan\", \"Allegan\", \"Allegan\",…\n$ EntityType      &lt;chr&gt; \"ISD\", \"ISD Non-Instructional Ancillary Facility\", \"IS…\n$ SCHOOL_LEVEL    &lt;chr&gt; \"Elem thru High School\", \"Other\", \"Other\", \"Elem thru …\n$ LOCALE_NAME     &lt;chr&gt; \"Town: Distant\", \"Not Specified\", \"Not Specified\", \"To…\n$ SCHOOL_EMPHASIS &lt;chr&gt; \"Not Applicable\", \"Not Applicable\", \"Not Applicable\", …\n$ SETTING         &lt;chr&gt; NA, NA, NA, \"Multiple Settings\", \"Multiple Settings\", …\n$ EMAIL_ADDRESS   &lt;chr&gt; \"william.brown@alleganaesa.org\", \"yhouser@alleganaesa.…\n$ PhoneNumber     &lt;dbl&gt; 2695127705, 2695127801, 2695127801, 2695127900, 269512…\n$ ADDRESS_LINE_1  &lt;chr&gt; \"310 Thomas St\", \"310 Thomas St\", \"310 Thomas St\", \"21…\n$ CITY            &lt;chr&gt; \"Allegan\", \"Allegan\", \"Allegan\", \"Allegan\", \"Allegan\",…\n$ STATE           &lt;chr&gt; \"MI\", \"MI\", \"MI\", \"MI\", \"MI\", \"MI\", \"MI\", \"MI\", \"MI\", …\n$ ZIP_CODE        &lt;dbl&gt; 49010, 49010, 49010, 49010, 49010, 49080, 49009, 49080…\n$ SCHOOL_TYPE     &lt;chr&gt; NA, NA, NA, \"Special Education\", \"Vocational/CTE\", NA,…\n$ Status          &lt;chr&gt; \"Open-Active\", \"Open-Active\", \"Open-Active\", \"Open-Act…\n\n\nThis is a flat dataset as it contains three observational units: Intermediate school districts (ISDs, also referred to as regional educational service agencies, RESAs), school districts, and buildings (typically schools but inclusive of administrative offices and other facilities). These three observational units are hierarchically nested, with ISDs containing districts, which contain schools. The “flatness” of the data is apparent as some rows represent ISDs and the buildings they operate outside of any one district (i.e., administrative offices, supplemental educational facilities). Other rows represent districts and their administrative offices. Generally each row represents a unique building with a distinct address, however some buildings are listed multiple times. This likely represents the co-location of different types of facilities (e.g., administrative offices located in a K-12 school). Some rows appear to be duplicated for no apparent reason ISDs themselves may contain one or more buildings directly, without an intervening district. Often this occurs because ISD administrative offices are located in their own building rather in one local districts. The first step Because the dataset is flat, summary statistics cannot easily be calculated for each level without using complex filtering conditions and grouping.\nWe won’t be using all these variables and will rename the relevant ones so that they are easier to work with. Also it appears that some closed buildings/schools are present, as well as a few duplicate rows present, so we will remove those (using filter() and unique()) before we start.\n\n\nCode\nmischool_flat &lt;- mischool_flat |&gt; \n  filter(Status == \"Open-Active\") |&gt; \n  distinct() |&gt;\n  select(\n    isd_code     = ISDCode,\n    isd_name     = ISDName,\n    dst_code     = DistrictCode,\n    dst_name     = DistrictName,\n    bld_code     = BuildingCode,\n    bld_name     = BuildingName,\n    bld_type     = SCHOOL_EMPHASIS,\n    bld_level    = SCHOOL_LEVEL,\n    ent_category = EntityType,\n    add_street   = ADDRESS_LINE_1,\n    add_city     = CITY,\n    add_county   = COUNTY_NAME,\n    add_zip      = ZIP_CODE\n  )"
  },
  {
    "objectID": "posts/learning_relational_databases_p2/index.html#identifying-key-variables",
    "href": "posts/learning_relational_databases_p2/index.html#identifying-key-variables",
    "title": "Relational Databases in R, Part 2",
    "section": "Identifying key variables",
    "text": "Identifying key variables\nGiven that there are three observational units, our relational database should include three tables. Each table will require a primary key and at least one foreign key. Our first step will be to establish which variables will serve as primary keys for each table. The only criterion for primary keys is that they have unique values for each observation in their table.\nThe tables below show that for isd_code, dst_code, and bld_code, not all values are unique in the flat format.\n\nCode\n# The occurence of each potential key variable is counted and filtered for values that occur more than once.\n# For a better presentation, counts are arranged in ascending order\n# kable(head(mischool_flat |&gt; count(isd_code) |&gt; filter(n &gt; 1) |&gt; arrange(n)))\n# kable(head(mischool_flat |&gt; count(dst_code) |&gt; filter(n &gt; 1) |&gt; arrange(n)))\n# kable(head(mischool_flat |&gt; count(sch_code) |&gt; filter(n &gt; 1) |&gt; arrange(n)))\nmischool_flat |&gt; count(isd_code) |&gt; filter(n &gt; 1) |&gt; arrange(n)\nmischool_flat |&gt; count(dst_code) |&gt; filter(n &gt; 1) |&gt; arrange(n)\nmischool_flat |&gt; count(bld_code) |&gt; filter(n &gt; 1) |&gt; arrange(n)\n\n\n\n\nTable 1: Examining potential key variables\n\n\n\n\n\n\n\n(a) ISD Code\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n(b) District Code\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n(c) Bulding Code\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nHowever, if we take the combination of isd_code, dst_code, and bld_code, we find that each row has a unique combined value (the number of distinct combinations, 4882, is the same as the total number of rows, 4882). Thus, it seems that isd_code, dst_code, and sch_code may be good key variables for the three tables that we want to construct."
  },
  {
    "objectID": "posts/learning_relational_databases_p2/index.html#creating-isd-district-and-school-tables",
    "href": "posts/learning_relational_databases_p2/index.html#creating-isd-district-and-school-tables",
    "title": "Relational Databases in R, Part 2",
    "section": "Creating ISD, District, and School Tables",
    "text": "Creating ISD, District, and School Tables\nThe administrative offices of ISDs and districts are indicated by a bld_code value of “00000”, according to the available documentation. Oftentimes, these entries duplicate the address (among other variables) of schools with students, representing how administrative offices often share a building with a K-12 school. Thus, bld_code != 00000 can be used as a filtering condition to separate out all entries representing schools.\nIn a similar manner, all ISD administrative offices appear to have a dst_code value that ends with three zeros. Thus, a condition with str_detect() can be constructed to separate out all entries representing district offices. Using these filtering conditions, we create our tables. Another check, shows that each table has a primary variable - as indicated by the fact there there are no duplicate values in the respective variables:\n\nCode\nmischool_database_sch &lt;- mischool_flat |&gt; \n  filter(bld_code != \"00000\")\n\nmischool_database_dst &lt;- mischool_flat |&gt; \n  filter(!str_detect(dst_code, \"000$\") & bld_code == \"00000\") |&gt; \n  select(\n    isd_code,\n    dst_code,\n    bld_code,\n    dst_name,\n    ent_category,\n    add_city,\n    add_county\n  )\n\nmischool_database_isd &lt;- mischool_flat |&gt;\n  filter(str_detect(dst_code, \"000$\") & bld_code == \"00000\") |&gt; \n  select(\n    isd_code,\n    isd_name,\n    dst_code,\n    bld_code,\n    ent_category,\n    add_county\n  )\n\n# A simpler way to determine if each variable is a good primary key is to find the number of values which appear more than once.\n# A primary key should never appear more than once!\n# mischool_database_sch |&gt; count(sch_code) |&gt; filter(n &gt; 1)\n# kable(head(mischool_database_dst |&gt; count(dst_code) |&gt; filter(n &gt; 1) |&gt; arrange(n)))\n# kable(head(mischool_database_isd |&gt; count(isd_code) |&gt; filter(n &gt; 1) |&gt; arrange(n)))\nmischool_database_sch |&gt; count(bld_code) |&gt; filter(n &gt; 1)\nmischool_database_dst |&gt; count(dst_code) |&gt; filter(n &gt; 1)\nmischool_database_isd |&gt; count(isd_code) |&gt; filter(n &gt; 1)\n\n\n\n\n ISD Code {#tbl-anonymous-8080511-1}\n  \n\n\n\n District Code {#tbl-anonymous-8080511-2}\n  \n\n\n\n School Code {#tbl-anonymous-8080511-3}\n  \n\n\n\n\n\nChecking primary key variables\n\n\n\nBelow is shown the school table which contains two additional variables isd_code and dst_code which act as foreign keys for the other two tables.\n\n\n School Table\n  \n\n\n\nWith our three tables, we have a relational database. Now, data can be added to each table and extracted for analysis in a consistent manner. Let’s start in the next post by adding some summary statistics that will make the database more useful.\n\nSaving the Database as Separate Tables\nIt is important to save this state of the database as it represents the end of the beginning of cleaning and restructuring of the MiSchool dataset. Reproducing this database will be very simple and if any breaking changes are introduced in the MiSchool dataset (changing variable names, dropped variables, etc.) then it will be easy to introduce fixes.\n\n\n\nCode\nfilepath &lt;- here(\"posts\", \"learning_relational_databases_p2\", \"data\", \"relational_database\")\n\nwrite_csv(mischool_database_sch, here(filepath, \"mischool_database_sch.csv\"))\nwrite_csv(mischool_database_dst, here(filepath, \"mischool_database_dst.csv\"))\nwrite_csv(mischool_database_isd, here(filepath, \"mischool_database_isd.csv\"))"
  },
  {
    "objectID": "posts/learning_extract_figure_data/index.html",
    "href": "posts/learning_extract_figure_data/index.html",
    "title": "Extracting Statistics From Figures in R",
    "section": "",
    "text": "I have been working on a meta-analysis project and ran into an annoying but all-too-common situation for meta-analysts. A few of the papers eligible for my project report very few summary or inferential statistics in text or in tables. Instead, results are reported in bar charts with means and standard errors. It is easy enough to eyeball values and get approximately correct figures, but values obtained in this way are difficult to reproduce.\n\n\n\n\n\n\n\n\n\n\n\nExample Graphs\n\n\n\nBack in my CHEM 101 days, I learned a couple important concepts in scientific measurement. First, decide upon a method or technique and stick to it. For example, when measuring out liquid in graduated cylinders chemists always spot values at the bottom of the meniscus rather than the top. Second, it is important to have some understanding of how precise you can be while maintaining accuracy and reproducibility. Reading down to 0.1mL on a 100mL graduate cylinder marked off in 1mL increments is probably not justifiable, but reading it to the nearest 0.5mL probably is.\nEyeballing values off a chart without a clear method is like taking scientific measurements with a common measuring cup. One can improve the measurement by taking additional steps (like overfilling, then scraping off the excess with knife) but these steps make an already time-consuming process more time consuming. Likewise, graphs and charts can be carefully annotated, zooming in and adding reference lines, but this is cumbersome and imprecise. What is needed is a software tool that minimizes effort and leads to reproducible values.\n\nTools for Data Extraction\nDifferent software tools exist that can digitize graphs. Some of these can automatically detect and extract data from The following are just those that I have tried to use:\n\nPlot Digitizer\nmetagear\njuicr\nmetaDigitise and shinyDigitise\n\nPlot Digitzer has been around for some time and can be downloaded as a free, standalone application. If I were to to use Plot Digitizer I would probably use the online app version which has a better GUI and allows for different chart types to be selected. Unfortunately, the most useful features are kept for a paid version (at least its a one-time payment!) and the free version simply allows coordinates to be selected and exported.\nThe other three are R packages that I have tried. metagear has several functions to facilitate systematic reviews and meta-analyses a few of which automatically extract data from select plot types (Lajeunesse 2016). These functions take the plot as an argument (it’s file path that is) and returns a dataframe with the detected data points. Unfortunately, this automatic data extraction has not worked for me in the few cases that I attempted. The defaults of the function can be changed to account for variations in plot style and quality, but after a couple hours of fiddling, I couldn’t make it work for me. Also, it hasn’t been updated since 2021 (as of May 2024) and it is unclear when it will receive improvements.\njuicr is an extension to the data extraction functions in metagear which adds a GUI and enables semi-automated functionality (Ivimey-Cook et al. 2023). Unfortunately, I ran into the same issues with the automatic data extraction as I did for metagear. juicr is in beta but it is unclear when it will receive further work, especially as it seems that only one person (Marc Lajeunesse) is contributing to its development. If this package does get more attention, I will definitely revisit it for future projects.\n\n\nmetaDigitise and shinyDigitise\nThe metaDigitise R package is strictly for manual data extraction, but it provides very useful guardrails that greatly reduce the potential for user error and saves annotated plots for reproducible results. This package was introduced in 2016 and continues to be actively maintained (Pick, Nakagawa, and Noble 2019). In 2022, the package authors introduced a shiny app that provides a GUI for a better user experience (Ivimey-Cook et al. 2023).\nThe workflow is quite simple and begins with a function call to initialize the shiny app. In that app the user is asked to provide a folder path where plot images are kept and whether they want to edit previous extractions or extract from new plots. The steps to complete data extraction are clearly shown and the user is able to go back and change options on previous steps.\n\n\n\nScreenshot of the shinyDigitise GUI with a new extraction in progress\n\n\nData extraction requires the user to select points on the image and what values those points are associated with. The examples in this blog post are mean/error plots but shinyDigitise works with five other plot types. The end result includes an annotated version of the plot and a dataframe of extracted values along with metadata, such as the file it was extracted from\n\n\n\nScreenshot of a complete extraction\n\n\nOne of the quality features is that the sample sizes of groups can be inputted which allows shinyDigitise to automatically calculate standard deviation from standard errors which are more commonly plotted.\n\n\nConclusion\nI hope this look at shinyDigitise and other tools for data extraction was useful Support for manual data extraction is very helpful and goes a long way towards reproducible effect size calculations for meta-analyses. I do look forward to the development of juicr and other tools that automate or semi-automate this process as this is still relatively time-intensive. My guess is that more collaborations between software developers and research synthesists are needed to develop such tools.\n\n\n\n\n\nReferences\n\nIvimey-Cook, Edward R., Daniel W. A. Noble, Shinichi Nakagawa, Marc J. Lajeunesse, and Joel L. Pick. 2023. “Advice for Improving the Reproducibility of Data Extraction in Meta-Analysis.” Research Synthesis Methods 14: 911–15.\n\n\nLajeunesse, Marc J. 2016. “Facilitating Systematic Reviews, Data Extraction and Meta-Analysis with the Metagear Package for r.” Methods in Ecology and Evolution 7 (3): 323–30. https://doi.org/10.1111/2041-210X.12472.\n\n\nPick, Joel L., Shinichi Nakagawa, and Daniel W. A. Noble. 2019. “Reproducible, Flexible and High-Throughput Data Extraction from Primary Literature: The metaDigitise r Package.” Methods in Ecology and Evolution 10 (3): 426–31. https://doi.org/10.1111/2041-210X.13118."
  },
  {
    "objectID": "posts/learning_relational_databases_p1/index.html",
    "href": "posts/learning_relational_databases_p1/index.html",
    "title": "Relational Databases in R, Part 1",
    "section": "",
    "text": "The first step in any data analysis workflow is ensuring that the data is clean and has a consistent, known structure. Analysis scripts that start with clean, consistent data are simpler, facilitate easy exploratory data analysis, and are extensible. There are at least several ways for data to be structured, but likely only one that is well-suited to the data analysis tools which will be used. The tidyverse tools in R - and other tools which follow “tidy data” conventions - are “tidy tools” meaning they take tidy data and output tidy data.\nHadley Wickham, one of the main developers of tidyverse packages, wrote a technical paper explaining the concepts of tidy data and tidy tools (Wickham 2014). Thankfully he also coauthored a very approachable book (Wickham, Çetinkaya-Rundel, and Grolemund 2023) in which tidy data is explained in simpler terms. In a few words, tidy data follows three principles:\nThe third principle may seem obvious but it becomes important in more complex data structures. Figure 1, from chapter 12, shows these three principles visually."
  },
  {
    "objectID": "posts/learning_relational_databases_p1/index.html#introduction-to-relational-databases",
    "href": "posts/learning_relational_databases_p1/index.html#introduction-to-relational-databases",
    "title": "Relational Databases in R, Part 1",
    "section": "Introduction to Relational Databases",
    "text": "Introduction to Relational Databases\nOften when data is collected, it will have a nested or hierarchical structure. This especially true for education data. For instance, students are nested in classrooms which are nested in schools, which are nested in districts … you get the point. At each level of nesting different variables will be relevant which affects data collection and analysis. For example, students have test scores and schools have average test scores - if all individual test scores are collected, it doesn’t make sense to contact the principle and collect the school average score. And yet we may still collect other, non-aggregated information about the school (public/sectarian, urban/rural, etc.) which are relevant to the analysis.\nEach level in a hierarchical dataset is called an observational unit. In order to get this data into a tidy format it will need to restructured so that each observational unit gets it own table. Remember the third principle of tidy data? The third principle may be expressed differently as “each value is expressed only once” (my words) or “each observational unit has its own table” (Wickham 2014). In the above example, all student observations would be collected in one table and all school observations in another.\nA set of tables are related to each using key variables, which allow the observations in one table to be related to observations in another. For example, in order to identify the school a student attends, a school_id variable is given to each student.\nThere are two types of keys. One primary key in included in each table to uniquely identify each observation. One or more foreign keys are included in each table in order to relate observations to those tables where the key is primary. So on the student table, “student_id” is the primary key and “school_id” is the foreign key, corresponding to the primary key on the school table."
  },
  {
    "objectID": "posts/learning_relational_databases_p1/index.html#why-use-relational-databases",
    "href": "posts/learning_relational_databases_p1/index.html#why-use-relational-databases",
    "title": "Relational Databases in R, Part 1",
    "section": "Why Use Relational Databases?",
    "text": "Why Use Relational Databases?\nFrom a quick reading of the Wikipedia page on the relational model (data science term for relational databases) there are alternative ways to structure nested data. What those alternatives are and what the relative theoretical advantages of the relational model are, I cannot say. (Perhaps that’s fodder for another post.) Relational databases do have the advantage of being both human- and machine-readable. This means that code can be easily written to manipulate a relational database and people can also open up an excel spreadsheet and peruse each table.\nWhen compared to “flat” data (all variables and values on one table), I have found two distinct advantages: interpretability and extensibility. The relational model represents a structural aspect of the system being investigated in the structure of the data. This enables much more intuitive assessments of the data and interpretations of the results than is possible with a flat structure. Additionally, a flat structure is constraining when a project evolves and more data is collected. Either new variables must added or existing variables must be renamed/redefined to incorporate the new observational unit. In a relational database, a new table and key variable(s) is simply added.\nTidyverse’s dplyr package has “two table verbs” which make it super easy to manipulate data structured as a relational database. One table verbs are functions that take one table as input, perform some operation, and output the resulting table. Two table verbs do the very same thing except they take two tables as input, require a matching variable (a key), and output a single table.\nIn part two, an example of a “flat” data file will be used to show how a relational database can be constructed. In part three (coming soon), our relational database will be used to generate summary statistics and extended to a fourth table."
  },
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "Papers",
    "section": "",
    "text": "This page is under construction \n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "This page is under construction \n\n\n\n\n\n\n\n\nNo matching items"
  }
]