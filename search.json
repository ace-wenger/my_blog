[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/learning_relational_databases_p3/index.html",
    "href": "posts/learning_relational_databases_p3/index.html",
    "title": "Creating and Using Relational Databases, Part 3",
    "section": "",
    "text": "blah blah"
  },
  {
    "objectID": "posts/learning_relational_databases_p3/index.html#adding-mistem-region-and-schooldistrict-counts",
    "href": "posts/learning_relational_databases_p3/index.html#adding-mistem-region-and-schooldistrict-counts",
    "title": "Creating and Using Relational Databases, Part 3",
    "section": "Adding MiSTEM Region and School/District Counts",
    "text": "Adding MiSTEM Region and School/District Counts\nUsing two-table verbs from dplyer (left_join() in the code below), district counts are easily added to the ISD table using isd_code as a key, matching each district to its corresponding ISD. Likewise, school counts are added to both ISD and district tables.\n\n\nCode\n## these are objects save in part 2\n# write_csv(mischool_database_sch, here(filepath, \"mischool_database_sch.csv\"))\n# write_csv(mischool_database_dst, here(filepath, \"mischool_database_dst.csv\"))\n# write_csv(mischool_database_isd, here(filepath, \"mischool_database_isd.csv\"))\n\n# mischool_database_isd &lt;- mischool_database_isd |&gt; \n#   left_join(\n#     mischool_database_dst |&gt; \n#       group_by(isd_code) |&gt; \n#       summarize(count_dst = n())\n#   ) |&gt; \n#   left_join(\n#     mischool_database_sch |&gt; \n#       group_by(isd_code) |&gt; \n#       summarize(count_sch = n())\n#   )\n# \n# mischool_database_dst &lt;- mischool_database_dst |&gt; \n#   left_join(\n#     mischool_database_sch |&gt; \n#       group_by(dst_code) |&gt; \n#       summarize(count_sch = n())\n#   )\n\n\n# select and display district/school counts\n\n\nIn Michigan, the MiSTEM network is a ———-. There are 15 MiSTEM regions ———. In order to add the MiSTEM region ————.\nHere’s the MiSTEM region directory ——-. And adding to databases"
  },
  {
    "objectID": "posts/learning_relational_databases_p3/index.html#adding-student-count-information",
    "href": "posts/learning_relational_databases_p3/index.html#adding-student-count-information",
    "title": "Creating and Using Relational Databases, Part 3",
    "section": "Adding Student Count Information",
    "text": "Adding Student Count Information"
  },
  {
    "objectID": "posts/learning_relational_databases_p3/index.html#creating-visualizations",
    "href": "posts/learning_relational_databases_p3/index.html#creating-visualizations",
    "title": "Creating and Using Relational Databases, Part 3",
    "section": "Creating Visualizations",
    "text": "Creating Visualizations"
  },
  {
    "objectID": "posts/learning_relational_databases_p1/index.html",
    "href": "posts/learning_relational_databases_p1/index.html",
    "title": "Creating and Using Relational Databases, Part 1",
    "section": "",
    "text": "Processing and structuring data for summary, statistical analysis, and visualization is critical for reproducible workflows. Analysis scripts that start with well-prepared data are cleaner, facilitate easy exploratory data analysis, and are extensible. But what does it mean for data to be “well-prepared?”\nWell-prepared data is structured with the data analysis tools in mind. The tidyverse tools in R - and other tools which follow “tidy data” conventions - are “tidy tools” meaning they take tidy data and output tidy data.\nHadley Wickham wrote a technical paper explaining the concepts of tidy data and tidy tools (Wickham 2014). Thankfully he also wrote a very approachable book that explains tidy data in simpler terms: R for Data Science.\nIn a few words, tidy data follows three principles:\nThe third principle may seem obvious but it becomes important in more complex data structures.  , from R for Data Science shows these three principles visually."
  },
  {
    "objectID": "posts/learning_relational_databases_p1/index.html#introduction-to-relational-databases",
    "href": "posts/learning_relational_databases_p1/index.html#introduction-to-relational-databases",
    "title": "Creating and Using Relational Databases, Part 1",
    "section": "Introduction to Relational Databases",
    "text": "Introduction to Relational Databases\nOften when data is collected, it will have a nested or hierarchical structure. This especially true for education data. For instance, students are nested in classrooms which are nested in schools, which are nested in districts … you get the point. At each level of nesting different variables will be relevant which affects data collection and analysis. For example, students have test scores and schools have average test scores - if all individual test scores are collected, it doesn’t make sense to contact the principle and collect the school average score. And yet we may still contact the principle and collect other pieces of information about the school.\nEach level in a hierarchical dataset is called an observational unit. In order to get this data into a tidy format it will need to restructured so that each observational unit gets it own table.\nRemember the third principle of tidy data? The third principle may be expressed differently as “each value is expressed only once” (my words) or “each observational unit has its own table” (Wickham 2014). Thus, each level of nestedness in the dataset - each observational unit - gets its own table. In the above example, all student observations are collected on one table and all school observations in another.\nA set of tables are related to each using key variables, which allow the observations in one table to be related to observations in another. For example, in order to identify the school a student attends, a school_id variable is given to each student.\nThere are two types of keys. One primary key in included in each table to uniquely identify each observation. One or more foreign keys are included in each table in order to relate observations to those tables that the key is primary. So on the student table, “student_id” is the primary key and “school_id” is the foreign key, corresponding to the primary key on the school table."
  },
  {
    "objectID": "posts/learning_relational_databases_p1/index.html#why-use-relational-databases",
    "href": "posts/learning_relational_databases_p1/index.html#why-use-relational-databases",
    "title": "Creating and Using Relational Databases, Part 1",
    "section": "Why Use Relational Databases?",
    "text": "Why Use Relational Databases?\nFrom a quick reading through of the Wikipedia page on the relational model (data science term for relational databases) there are alternative ways to structure nested data. What those alternatives are and what the relative theoretical advantages of the relational model are, I cannot say. Perhaps that’s fodder for another post.\nRelational databases do seem to have the advantage of being human- and machine-readable. This means that code can be easily written to manipulate a relational database and people can also open up an excel spreadsheet and peruse each table.\nTidyverse’s dplyr package has “two table verbs” which make it super easy to manipulate data structured as a relational database. One table verbs are functions that take one table as input, perform some operation, and output the resulting table. Two table verbs do the very same thing except they take two tables as input and require a matching variable, a key.\nIn the next post  an example of a “flat” data file will be used to show how a relational database can be constructed and used to generate summary statistics."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "my_blog",
    "section": "",
    "text": "Extracting Statistics From Figures in R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearing renv for Reproduciblity\n\n\n… and other benefits\n\n\n\nresearch reproducibility\n\n\nlearning R\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\nAaron Wenger\n\n\n\n\n\n\n\n\n\n\n\n\nCreating and Using Relational Databases, Part 2\n\n\n\n\n\n\nrelational databases\n\n\nlearning R\n\n\ndata science\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nJun 23, 2023\n\n\nAaron Wenger\n\n\n\n\n\n\n\n\n\n\n\n\nCreating and Using Relational Databases, Part 3\n\n\n\n\n\n\nrelational databases\n\n\nlearning R\n\n\ndata science\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nJun 23, 2023\n\n\nAaron Wenger\n\n\n\n\n\n\n\n\n\n\n\n\nCreating and Using Relational Databases, Part 1\n\n\n\n\n\n\nrelational databases\n\n\nlearning R\n\n\ndata science\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nMay 13, 2023\n\n\nAaron Wenger\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMay 13, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/learning_extract_figure_data/index.html",
    "href": "posts/learning_extract_figure_data/index.html",
    "title": "Extracting Statistics From Figures in R",
    "section": "",
    "text": "I have been working on a meta-analysis project and ran into an annoying but all-too-common situation for meta-analysts. A few of the papers eligible for my project report very few summary or inferential statistics in text or in tables. Instead, results are reported in bar charts with means and standard errors. It is easy enough to eyeball values and get approximately correct figures, but values obtained in this way are difficult to reproduce.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample Graphs\n\n\n\nBack in my CHEM 101 days, I learned a couple important concepts in scientific measurement. First, decide upon a method or technique and stick to it. For example, when measuring out liquid in graduated cylinders chemists always spot values at the bottom of the meniscus rather than the top. Second, it is important to have some understanding of how precise you can be while maintaining accuracy and reproducibility. Reading down to 0.1mL on a 100mL graduate cylinder marked off in 1mL increments is probably not justifiable, but reading it to the nearest 0.5mL probably is.\nEyeballing values off a chart without a clear method is like taking scientific measurements with a common measuring cup. One can improve the measurement by taking additional steps (like overfilling, then scraping off the excess with knife) but these steps make an already time-consuming process more time consuming. Likewise, graphs and charts can be carefully annotated, zooming in and adding reference lines, but this is cumbersome and imprecise. What is needed is a software tool that minimizes effort and leads to reproducible values.\n\nTools for Data Extraction\nDifferent software tools exist that can digitize graphs. Some of these can automatically detect and extract data from The following are just those that I have tried to use:\n\nPlot Digitizer\nmetagear\njuicr\nmetaDigitise and shinyDigitise\n\nPlot Digitzer has been around for some time and can be downloaded as a free, standalone application. If I were to to use Plot Digitizer I would probably use the online app version which has a better GUI and allows for different chart types to be selected. Unfortunately, the most useful features are kept for a paid version (at least its a one-time payment!) and the free version simply allows coordinates to be selected and exported.\nThe other three are R packages that I have tried. metagear has several functions to facilitate systematic reviews and meta-analyses a few of which automatically extract data from select plot types (Lajeunesse 2016). These functions take the plot as an argument (it’s file path that is) and returns a dataframe with the detected data points. Unfortunately, this automatic data extraction has not worked for me in the few cases that I attempted. The defaults of the function can be changed to account for variations in plot style and quality, but after a couple hours of fiddling, I couldn’t make it work for me. Also, it hasn’t been updated since 2021 (as of May 2024) and it is unclear when it will receive improvements.\njuicr is an extension to the data extraction functions in metagear which adds a GUI and enables semi-automated functionality (Ivimey-Cook et al. 2023). Unfortunately, I ran into the same issues with the automatic data extraction as I did for metagear. juicr is in beta but it is unclear when it will receive further work, especially as it seems that only one person (Marc Lajeunesse) is contributing to its development. If this package does get more attention, I will definitely revisit it for future projects.\n\n\nmetaDigitise and shinyDigitise\nThe metaDigitise R package is strictly for manual data extraction, but it provides very useful guardrails that greatly reduce the potential for user error and saves annotated plots for reproducible results. This package was introduced in 2016 and continues to be actively maintained (Pick, Nakagawa, and Noble 2019). In 2022, the package authors introduced a shiny app that provides a GUI for a better user experience (Ivimey-Cook et al. 2023).\nThe workflow is quite simple and begins with a function call to initialize the shiny app. In that app the user is asked to provide a folder path where plot images are kept and whether they want to edit previous extractions or extract from new plots. The steps to complete data extraction are clearly shown and the user is able to go back and change options on previous steps.\n\n\n\nScreenshot of the shinyDigitise GUI with a new extraction in progress\n\n\nData extraction requires the user to select points on the image and what values those points are associated with. The examples in this blog post are mean/error plots but shinyDigitise works with five other plot types. The end result includes an annotated version of the plot and a dataframe of extracted values along with metadata, such as the file it was extracted from\n\n\n\nScreenshot of a complete extraction\n\n\nOne of the quality features is that the sample sizes of groups can be inputted which allows shinyDigitise to automatically calculate standard deviation from standard errors which are more commonly plotted.\n\n\nConclusion\nI hope this look at shinyDigitise and other tools for data extraction was useful Support for manual data extraction is very helpful and goes a long way towards reproducible effect size calculations for meta-analyses. I do look forward to the development of juicr and other tools that automate or semi-automate this process as this is still relatively time-intensive. My guess is that more collaborations between software developers and research synthesists are needed to develop such tools.\n\n\n\n\n\nReferences\n\nIvimey-Cook, Edward R., Daniel W. A. Noble, Shinichi Nakagawa, Marc J. Lajeunesse, and Joel L. Pick. 2023. “Advice for Improving the Reproducibility of Data Extraction in Meta-Analysis.” Research Synthesis Methods 14: 911–15.\n\n\nLajeunesse, Marc J. 2016. “Facilitating Systematic Reviews, Data Extraction and Meta-Analysis with the Metagear Package for r.” Methods in Ecology and Evolution 7 (3): 323–30. https://doi.org/10.1111/2041-210X.12472.\n\n\nPick, Joel L., Shinichi Nakagawa, and Daniel W. A. Noble. 2019. “Reproducible, Flexible and High-Throughput Data Extraction from Primary Literature: The metaDigitise r Package.” Methods in Ecology and Evolution 10 (3): 426–31. https://doi.org/10.1111/2041-210X.13118."
  },
  {
    "objectID": "posts/learning_relational_databases_p2/index.html",
    "href": "posts/learning_relational_databases_p2/index.html",
    "title": "Creating and Using Relational Databases, Part 2",
    "section": "",
    "text": "The following example uses data on Michigan K-12 public schools to demonstrate how easy it can be to get “flat” data into a relational database format and quickly produce summary statistics."
  },
  {
    "objectID": "posts/learning_relational_databases_p2/index.html#introduction-to-dataset",
    "href": "posts/learning_relational_databases_p2/index.html#introduction-to-dataset",
    "title": "Creating and Using Relational Databases, Part 2",
    "section": "Introduction to Dataset",
    "text": "Introduction to Dataset\nThe first dataset used in this example was downloaded from the Michigan government site MI School Data. It holds contact information and various other data points for all current Michigan public educational entities. All 22 variables are shown with the dplyr::glimpse() function:\n\n\nRows: 5,010\nColumns: 22\n$ SchoolYear      &lt;chr&gt; \"22 - 23 School Year\", \"22 - 23 School Year\", \"22 - 23…\n$ ISDCode         &lt;chr&gt; \"03\", \"03\", \"03\", \"03\", \"03\", \"03\", \"03\", \"03\", \"03\", …\n$ ISDName         &lt;chr&gt; \"Allegan Area Educational Service Agency\", \"Allegan Ar…\n$ DistrictCode    &lt;chr&gt; \"03000\", \"03000\", \"03000\", \"03000\", \"03000\", \"03010\", …\n$ DistrictName    &lt;chr&gt; \"Allegan Area Educational Service Agency\", \"Allegan Ar…\n$ BuildingCode    &lt;chr&gt; \"00000\", \"03816\", \"03816\", \"06730\", \"07254\", \"00000\", …\n$ BuildingName    &lt;chr&gt; \"Allegan Area Educational Service Agency\", \"Early Coll…\n$ COUNTY_CODE     &lt;chr&gt; \"03\", \"03\", \"03\", \"03\", \"03\", \"03\", \"39\", \"03\", \"03\", …\n$ COUNTY_NAME     &lt;chr&gt; \"Allegan\", \"Allegan\", \"Allegan\", \"Allegan\", \"Allegan\",…\n$ EntityType      &lt;chr&gt; \"ISD\", \"ISD Non-Instructional Ancillary Facility\", \"IS…\n$ SCHOOL_LEVEL    &lt;chr&gt; \"Elem thru High School\", \"Other\", \"Other\", \"Elem thru …\n$ LOCALE_NAME     &lt;chr&gt; \"Town: Distant\", \"Not Specified\", \"Not Specified\", \"To…\n$ SCHOOL_EMPHASIS &lt;chr&gt; \"Not Applicable\", \"Not Applicable\", \"Not Applicable\", …\n$ SETTING         &lt;chr&gt; NA, NA, NA, \"Multiple Settings\", \"Multiple Settings\", …\n$ EMAIL_ADDRESS   &lt;chr&gt; \"william.brown@alleganaesa.org\", \"yhouser@alleganaesa.…\n$ PhoneNumber     &lt;dbl&gt; 2695127705, 2695127801, 2695127801, 2695127900, 269512…\n$ ADDRESS_LINE_1  &lt;chr&gt; \"310 Thomas St\", \"310 Thomas St\", \"310 Thomas St\", \"21…\n$ CITY            &lt;chr&gt; \"Allegan\", \"Allegan\", \"Allegan\", \"Allegan\", \"Allegan\",…\n$ STATE           &lt;chr&gt; \"MI\", \"MI\", \"MI\", \"MI\", \"MI\", \"MI\", \"MI\", \"MI\", \"MI\", …\n$ ZIP_CODE        &lt;dbl&gt; 49010, 49010, 49010, 49010, 49010, 49080, 49009, 49080…\n$ SCHOOL_TYPE     &lt;chr&gt; NA, NA, NA, \"Special Education\", \"Vocational/CTE\", NA,…\n$ Status          &lt;chr&gt; \"Open-Active\", \"Open-Active\", \"Open-Active\", \"Open-Act…\n\n\nThis is a “flat” dataset as it contains three observational units: Intermediate school districts (ISDs), school districts, and schools (referred to as “buildings”). These three observational units are hierarchically nested with ISDs - also known as regional educational service agencies (RESAs) - containing districts, which contain schools. ISDs themselves may contain one or more schools directly, without an intervening district. Because the dataset is flat, summary statistics cannot easily be calculated for each level without using complex filtering conditions and grouping.\nWe won’t be using all these variables and will rename the relevant ones so that they are easier to work with. Also it appears that some closed buildings/schools are present, as well as a few duplicate rows present, so we will remove those (using filter() and unique()) before we start.\n\n\nCode\nmischool_flat &lt;- mischool_flat |&gt; \n  filter(Status == \"Open-Active\") |&gt; \n  distinct() |&gt;\n  select(\n    isd_code     = ISDCode,\n    isd_name     = ISDName,\n    dst_code     = DistrictCode,\n    dst_name     = DistrictName,\n    sch_code     = BuildingCode,\n    sch_name     = BuildingName,\n    sch_type     = SCHOOL_EMPHASIS,\n    sch_level    = SCHOOL_LEVEL,\n    ent_category = EntityType,\n    add_street   = ADDRESS_LINE_1,\n    add_city     = CITY,\n    add_county   = COUNTY_NAME,\n    add_zip      = ZIP_CODE\n  )"
  },
  {
    "objectID": "posts/learning_relational_databases_p2/index.html#identifying-key-variables",
    "href": "posts/learning_relational_databases_p2/index.html#identifying-key-variables",
    "title": "Creating and Using Relational Databases, Part 2",
    "section": "Identifying key variables",
    "text": "Identifying key variables\nGiven that there are three observational units, our relational database should include three tables. Each table will require a primary key and at least one foreign key. Our first step will be to establish which variables will serve as primary keys for each table. The only criterion for primary keys is that they have unique values for each observation in their table.\nThe tables below show that for isd_code, dst_code, and sch_code, not all vales are unique in the flat dataset.\n\nCode\n# The occurence of each potential key variable is counted and filtered for values that occur more than once.\n# For a better presentation, counts are arranged in ascending order\n# kable(head(mischool_flat |&gt; count(isd_code) |&gt; filter(n &gt; 1) |&gt; arrange(n)))\n# kable(head(mischool_flat |&gt; count(dst_code) |&gt; filter(n &gt; 1) |&gt; arrange(n)))\n# kable(head(mischool_flat |&gt; count(sch_code) |&gt; filter(n &gt; 1) |&gt; arrange(n)))\nmischool_flat |&gt; count(isd_code) |&gt; filter(n &gt; 1) |&gt; arrange(n)\n\n\nCode\nmischool_flat |&gt; count(dst_code) |&gt; filter(n &gt; 1) |&gt; arrange(n)\n\n\nCode\nmischool_flat |&gt; count(sch_code) |&gt; filter(n &gt; 1) |&gt; arrange(n)\n\n\n\n\nTable 1: Examining potential key variables\n\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\nHowever, if we take the combination of isd_code, dst_code, and sch_code, we find that each row has a unique combined value (the number of distinct combinations, 4882, is the same as the total number of rows, 4882). Thus, it seems that isd_code, dst_code, and sch_code may be good key variables for the three tables that we want to construct."
  },
  {
    "objectID": "posts/learning_relational_databases_p2/index.html#creating-isd-district-and-school-tables",
    "href": "posts/learning_relational_databases_p2/index.html#creating-isd-district-and-school-tables",
    "title": "Creating and Using Relational Databases, Part 2",
    "section": "Creating ISD, District, and School Tables",
    "text": "Creating ISD, District, and School Tables\nFrom an examination of the dataset and available documentation, it can determined that the administrative offices of ISDs and districts are indicated by a sch_code value of “00000.” Oftentimes, these entries duplicate the address (among other variables) of schools with students, representing how administrative offices often share a building with a K-12 school. Thus, sch_code != 00000 can be used as a filtering condition to separate out all entries representing schools.\nIn a similar manner, all ISD administrative offices appear to have a dst_code value that ends with three zeros. Thus, a condition with str_detect() can be constructed to separate out all entries representing district offices. Using these filtering conditions, we create our tables. Another check, shows that each table has a primary variable:\nmischool_database_sch &lt;- mischool_flat |&gt; \n  filter(sch_code != \"00000\")\n\nmischool_database_dst &lt;- mischool_flat |&gt; \n  filter(!str_detect(dst_code, \"000$\") & sch_code == \"00000\") |&gt; \n  select(\n    isd_code,\n    dst_code,\n    sch_code,\n    dst_name,\n    ent_category,\n    add_city,\n    add_county\n  )\n\nmischool_database_isd &lt;- mischool_flat |&gt;\n  filter(str_detect(dst_code, \"000$\") & sch_code == \"00000\") |&gt; \n  select(\n    isd_code,\n    isd_name,\n    dst_code,\n    sch_code,\n    ent_category,\n    add_county\n  )\n\n# A simpler way to determine if each variable is a good primary key is to find the number of values which appear more than once.\n# A primary key should never appear more than once!\n# mischool_database_sch |&gt; count(sch_code) |&gt; filter(n &gt; 1)\n# kable(head(mischool_database_dst |&gt; count(dst_code) |&gt; filter(n &gt; 1) |&gt; arrange(n)))\n# kable(head(mischool_database_isd |&gt; count(isd_code) |&gt; filter(n &gt; 1) |&gt; arrange(n)))\nmischool_database_sch |&gt; count(sch_code) |&gt; filter(n &gt; 1)\nmischool_database_dst |&gt; count(dst_code) |&gt; filter(n &gt; 1)\nmischool_database_isd |&gt; count(isd_code) |&gt; filter(n &gt; 1)\n\n\n\nCode\n\n\n ISD Code {#tbl-anonymous-697837-1}\n  \n\n\n\nCode\n\n\n\n\n District Code {#tbl-anonymous-697837-2}\n  \n\n\n\nCode\n\n\n School Code {#tbl-anonymous-697837-3}\n  \n\n\n\n\n\nChecking primary key variables\n\n\n\nBelow is shown the school table which contains two additional variables isd_code and dst_code which act as foreign keys for the other two tables.\n\n\n School Table\n  \n\n\n\nWith our three tables, we have a relational database. Now, data can be added to each table and extracted for analysis in a consistent manner. Let’s start in the next post by adding some summary statistics that will make the database more useful.\n\nSaving the Database\nIt is important to save this state of the database as it represents the end of our cleaning and restructuring of the MiSchool dataset. Reproducing this database will be very simple and if any breaking changes are introduced in the MiSchool dataset (changing variable names, dropped variables, etc.) then it will be easy to introduce fixes.\n\n\nCode\nfilepath &lt;- here(\"posts\", \"learning_relational_databases_p2\", \"data\", \"relational_database\")\n\nwrite_csv(mischool_database_sch, here(filepath, \"mischool_database_sch.csv\"))\nwrite_csv(mischool_database_dst, here(filepath, \"mischool_database_dst.csv\"))\nwrite_csv(mischool_database_isd, here(filepath, \"mischool_database_isd.csv\"))"
  },
  {
    "objectID": "posts/learning_renv/index.html",
    "href": "posts/learning_renv/index.html",
    "title": "Untitled",
    "section": "",
    "text": "Organizing, managing, and executing reproducible research projects is hard because there are so many possible reasons why a project cannot be reproduced.\n- No data? Not reproducible.\n- No code? Not reproducible.\n- Software is unavailable? Not reproducible.\n- Computational environment is unspecified? Maybe.\n- Software versions are unknown? Eventually no.\nWe think that we have one of these threats to reproducibility whipped, then some unexpected development or complication throws it all back in doubt. I think this is especially true for someone like me: an intermediate R user who has no background in computer science or programming other than what I have learned on my own in the last couple years. This post reflects on my learning how to use renv as part of a reproducible research workflow and how I have handled a couple problems along the way.\n\nWelcome renv\nThe renv package solves (more or less) the software versioning issue. With a couple commands all R package versions are recorded in a file, allowing the R package environment of the project to be rebuilt from scratch. I have used renv for several months now and have appreciated how much it has simplified the task of installing and staying current with package updates. It has also streamlined the storage of R packages on my computer.\nIn a nutshell, here’s how it works. Once renv is installed init() creates a lockfile (renv.lock) that holds package version information and details about how packages were installed. For example, if a package is installed from github, that is recorded along with the username of the repository. When the project is shared, the lockfile can be read by restore() to install all packages exactly as they are recorded. The snapshot() function adds additional packages as they are installed in the project and status() reports packages that need to be installed and/or added to the lockfile. It’s really that easy!\n\n\nFirst Headache\nLearning to use renv and incorporating it into my workflow wasn’t without a couple headaches. At the start, I couldn’t use renv::install()which works similarly to baserenv::install.packages()but is more flexible and intuitive. I would runrenv::install(\"somepackage\") and an error would be returned, to the effect that “package ‘somepackage’ is not available” Yet utils::install.packages(), the “base” package installation function (utils is part of the R distribution), worked just fine.\nApparently many others have had this problem. What it came down to for me is that R and renv were using different download methods. These methods can be checked using getOption(\"download.file.method\") and renv:::renv_download_method() for R and renv respectively. It seems for me (on Windows) that the two available methods are lib and libcurl. These are closely related software libraries/tools created and maintained by the cURL (Client for URLs) project which enable internet file transfers.\nI resolved this problem by including one line in my Rstudio project .Rprofile file: Sys.setenv(RENV_DOWNLOAD_FILE_METHOD = \"libcurl\"). Being in the .Rprofile within the project, this command is always run when loading up the project in Rstudio. A more robust solution that dynamically retrieves the download method currently used by R is what I use now: Sys.setenv(RENV_DOWNLOAD_METHOD = getOption(\"download.file.method\")) (see this stackoverflow question)\n\n\nSecond Headache\nThat first problem seemed to happen again some months later. Again, renv::install() would return an error stating that the package was not available. Apparently, the repository being targeted by renv was the issue. I don’t know what caused it, but I am guessing some update in the backend of renv or its dependencies was responsible.\nThe call getOption(\"repos\") returns the repository currently being used which for me was something like https://packagemanager.posit.co/cran/. I resolved the problem by manually setting the “repos” option in my project’s .Rprofile file. Thus after solving these two headaches, I start new Rstudio projects using renv with my .Rprofile looking like this:\n\nsource(\"renv/activate.R\")\n\nSys.setenv(RENV_DOWNLOAD_FILE_METHOD = getOption(\"download.file.method\"))\n\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\n\n\n\nUnanticipated Benefits\nThe above problems were really not too hard to resolve even for someone with a limited software knowledge base like me. The documentation and support provided by Kevin Ushey, principal developer of renv, and the rest of the Posit team really is superb. I have no doubt that this package will remain stable and functioning going ahead.\nAs I have adopted renv into my normal project workflow I have discovered a few benefits beyond supporting research reproducibility. First, the renv::install() and renv::update() functions are very smooth and intuitive, much better than utils::install.packages(). In particular, I run renv::update() every week or month to automatically update all project packages - including renv itself! Both functions are faster than the utils function and provide more informative errors.\nA second benefit is the efficient caching of R packages on my computer. After the first hundred or so installed packages, the disk space required becomes noticeable. Before renv, I occasionally installed packages twice in different locations and accumulated different versions over time. renv keeps a common cache for the device which it then links to in individual renv projects. This means that for a given package and version, it will only ever be installed once. My computer only has a 200GB hard drive so saving a GB here or there is very nice.\nA third benefit is the quick installation of new-to-me packages. Often dependencies are already included and built in my cache so all renv has to do is link to the cache for that dependency. Thus, downloads and builds from binaries are minimized and new packages are ready to use in mere moments.\nI think all scientists and researchers who use R and who are committed to research reproducibility should adopt renv into their workflow."
  }
]